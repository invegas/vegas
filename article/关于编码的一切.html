<!DOCTYPE html>
<html>
<head>
	<title>谈javascript变量声明</title>
	<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
	<link rel="stylesheet" type="text/css" href="prettify/sunburst.css">
	<script type="text/javascript" src="prettify/prettify.js"></script>	
	<style type="text/css">
		body {
			font-family: '微软雅黑';
		}
		blockquote p{
			font-style: italic;
			font-size: 12px;
		}
	</style>
</head>
<body onload="prettyPrint()">
	<div class="container">
		<div class="row">
			<div class="span1"></div>
			<div class="span6">
				<h3>Javascript: 你真的了解函数声明吗？</h3>
			</div>
			<div class="span4"></div>
			<div class="span1"></div>
		</div>		
		<div class="row">
			<div class="span1"></div>
			<div class="span6">




			<p>有感于，我们每天用各种的编辑器，嘴里喊着utf-8，BOM头，gbk，encode，decode，却鲜有人知道它们的由来和为什么这样做（好吧，也有可能就我一个人不知道）。最近找了很多资料，在这里做一个整理，和大家分享。</p>
			<strong>关于Unicode,UTF8,Character Sets的前生今世</strong>
			<strong>ASC II</strong>
			<p>Long long time age, 总所周知计算机只能处理数字而不能处理字符，所以把哪些字符由哪些数字表示统一起来（而不是每台计算机有每台计算机的标准）非常重要。</p>
			<p>比如说我的计算机用1表示A，2表示B，3表示C，诸如此类。而你的计算机用0表示A，1表示B，2表示C等等。那么当我发送给你一条内容为“HELLO”的消息时，数字8,5,12,12,15就通过光缆传输到了你那，但因为数字表示的字符不同，当你收到这串数字时，你会把它翻译(<strong>decode</strong>)成“IFMMP”。为了有效的交流，我们必须对如何编码(encoding)这些字符达成一致。</p>
			<p>终于，在十九世纪60年代，美国标准协会(American Standards Association)发明了7位(7-bit)编码方式，称为美国信息交换标准码(American Standard Code for Information Interchange)，就是我们熟知的<strong>ASCII</strong>。在这种编码标准下，HELLO表示的数字是72,69,76,76,79，并且会以二进制1001000 1000101 1001100 1001100 1001111的形式进行传输。7位编码一共提供了128种可能，从0000000到1111111。那时所有的拉丁字符
			的大小写，通用的标点，缩进，空格和其它一些控制符都能在ASCII中有一席之地。在1968年，美国总统Lyndon Johnson宣布所有的计算机必须使用和能读懂ASCII标准，ASCII成为官方标准。</p>
			<strong>8-bit</strong>
			<p>电报一类的工具当然乐于使用七位编码去传输信息，但是到了七十年代，计算机的处理器更乐意与2的次方打交道——他们已经可以用8位来存储字符，也就是说这将提供256种可能。</p>
			<p>一个8位字符被存储的数字最大是255，但是ASCII标准最大只到127。这就意味着从128到255被空了出来。IBM用这些多余的数字来存储一些形状，希腊字母。比如200代表一个盒子的左下角╚，244代表小写的希腊字母<strong>α</strong>。这种编码方式的所有字符编码都列在<a target="_blank" href="http://en.wikipedia.org/wiki/Code_page_437">代码页(Code page)437中</a></p>
			<p>无论如何，不像ASCII标准，128至255的字符序列从来都没有被标准化。不同国家都用自己的字母表来填充这些多余的序列。不是所人都同意224代表<strong>α</strong>，甚至希腊人自己都有分歧，因为在希腊另一代码页737中，224代表小写<strong>ω</strong>。相当数量的新的代码页的出现，比如俄罗斯的IBM计算机使用的是代码页885,224代表的是西里尔(Cyrillic)字母<strong>Я</strong>。</p>
			<p>在存在分歧的情况下，十九世纪八十年代微软也推出了自己的代码页，在西里尔文代码页Windows-1251中，224代表西里尔文字母<strong>a</strong>，而之前的<strong>Я</strong>的位置是223.</p>
			<p>到了九十年代末期，大家做了一次标准化的尝试。15种8位字符集被推出，涵盖不同的字母表，比如西里尔文，阿拉伯文，希伯来文，土耳其文和泰文。它们被称为<a href="" target="_blank"> ISO-8859-1 up to ISO-8859-16</a>(字母12被抛弃)。在西里尔标准ISO-8859-5中，224代表字母<strong>р</strong>，而<strong>Я</strong>的位置是207.
			</p>
			<p>如果一位俄罗斯朋友发给你一份文档，你必须知道他使用的是哪一种字符集。文档本身只是数字序列而已，224代表的可能是Я,a 或者р。如果用错了字符集打开这份文档，那会是非常糟糕的一件事。</p>
			<strong>给1990年做个总结</strong>
			<p>在1990年左右的情况大致是这样的，文档可以用不同的语言书写，保存，并且在不同语言间交换。但是<strong>你必须得知道他们用的是哪一种字符集</strong>。当然更不可能在一份文档中用多种语言。像中文和日文只能用完全不同的编码体系。
			</p>
			<p>终于，互联网出现了！国际化和全球化让这个问题被放的更大，一个新的标准亟需出现。</p>
			<strong>Unicode来拯救人类了</strong>
			<p>从八十年代后期开始，一种新的标准已经被提出。它能给每一种语言中的每一个字符赋予唯一的标识，当然远远大于256.它被称为<strong>Unicode</strong>，至今为止它的版本是6.1，包括了超过110000个字符。</p>
			<p>Unicode的头128个字符与ASCII一致。128至255则包含了货币符号，常用符号，还有<a target="_blank" href="http://en.wikipedia.org/wiki/Diacritic">附加符号和变音符(accented characters)</a>。大部分都是借鉴自ISO-8859-1。在编号256之后，还有更多的变音符。在编号880之后开始进入希腊文字符集，然后是西里尔文，希伯来文，阿拉伯文，印度语，泰文。中文，日文和韩文从11904开始，其中也夹杂了其他的语言。</p>
			<p>毫不含糊的说这的确是一件福利，每一个字符都由属于自己独一无二的数字表示。西里尔文的Я永远是1071，希腊文的α永远是945.而224永远是à，H任然是72.注意官方书写的Unicode编码是以U+为开头的十六进制表示。所以H的正确写法应该是U+48而不是72（把十六进制转化为十进制：4*16+8=72）</p>
			<p>最主要的问题是超出256的那部分。想当然8位已经容不下这些字符了。但无论如何Unicode并不是一个字符集或者是代码页。所以这也不是Unicode制定协会的问题。他们只是给出了一个解决问题的想法而剩下实际操作的问题则留给其他人去办了。下两节我们会讨论这个问题。</p>
			<strong>浏览器中的Unicode</strong>
			<p>8位已经容不下Unicode了，甚至16也已经容不下，虽然只有110116个字符真正被使用，但是已经定义字符已经升至了1114112个，这就意味着至少需要21位</p>
			<p>从七十年代开始计算机已经变得非常先进了。八位的处理器早就过时了。现在的计算机已经拥有64位的处理器，所以我们为什么不可以把超出8位容纳范围的字符转移至32位或64位呢</p>
			<p>我们当然可以这么做！</p>
			<p>大部分的软件都是用C或者C++完成的，这两种语言支持一种名为"wide character"的数据类型。这种32为的字符数据类型被称为<code>wchar_t</code>，它是对C语言的8位数据类型<code>char</code>的一种拓展。</p>
			<p>从本质上来说，现代浏览器完全可以使用上面所说的这种字符类型，理论上它们就可以毫无压力的处理超过40亿个完全不同的字符，这对Unicode来说是莫大的喜讯——<strong>理论上，现代浏览器是可以使用Unicode的</strong></p>
			<strong>UTF-8又来拯救人类了</strong>
			<p>既然浏览器可以应付32位的Unicode字符，那么还有什么问题？<strong>现在的瓶颈是，字符的传输和读写。</strong></p>
			<p>使用Unicode的障碍仍然存在是因为:</p>
			<ul>
				<li>相当数量的现存软件和各种协议都只能传输和读写8位字符</li>
				<li>使用32位来存储或发送英文内容会让带宽变为原来的四倍</li>
			</ul>
			<p>虽然浏览器内部对处理Unicode来说毫无压力，但你仍需从服务器端获取数据或者发送数据到服务器，你也需要把数据存在文件或者数据库中。所以你仍然需要用8位来存储110000个字符。</p>
			<p><strong>UTF-8</strong>用一种很睿智的方式办到了。它的工作原理类似于键盘上的shift换挡键，一般来说你按下键盘上的H键打印出的是小写的h，但是如果按住shift键的同时按下H键，你就能打印出大写H</p>
			<p>UTF-8把0至127留给ASCII，192至247作为换挡键(key)，128只192作为被换挡的对象。举个例子，使用字符208和209能切换进入西里尔文字符范围，字符175后跟换挡键208对于字符1071，也就是西里尔文中的Я，具体的计算方法是 (208%32)*64 + (175%64) = 1071。字符224至239是双倍换挡键。<a href="http://www.fileformat.info/info/unicode/char/2F80/index.htm" target="_blank">字符190跟换挡键226，在加上字符128对应字符12160:</a>&#12160; 字符240和之后的换挡键是三倍换挡键。</p>
			<p>因此UTF-8被称为多字节(multi-byte)可变宽度(variable-width)编码，称之为多字节是因为比如Я这种的字符需要不止一个字节标识它，称之为可变宽度是因为一些像H这样的字符可能需要1个字节也可能需要4个字节。</p>
			<p>Unicode还有一个好处在于它向后兼容ASCII。不像其他的一些解决方案，所以用纯ASCII编码的文档，也都能转为有效的UTF-8编码。这大大的解决了带宽和转化的麻烦。</p>
			<strong>给2003年做一个总结</strong>
			<p>UTF-8已经称为互联网最受欢迎的国际字符集。但当你浏览一份非英语语言文档时，你仍然需要知道它用的是什么字符集。为了能让所有的网页能最大范围的流通，网页所有者们必须保证他们的网页使用的是UTF-8编码。</p>
			<strong>许许多多的问题</strong>
			<p>如果每一个人使用都是UTF-8编码——非常好，天下太平。但事实并非如此，这便导致了一些混乱，想象一下一个非常典型的操作，一个用户给一篇博客添加评论：</p>
			<ol>
				<li>网页显示一个供提交评论的表单</li>
				<li>用户填写评论并且提交</li>
				<li>评论被提交至服务器并且存在数据库中</li>
				<li>该评论可能会从数据库中读出来并且展现在页面上</li>
			</ol>
			<p>就这简简单单的流程会以很多方式走岔了，产生了以下这些问题：</p>
			<strong>HTML实体集(ENTITIES)</strong>
			<p>假设你对字符集一无所知，刚刚半小时看的东西全忘了。上面提到的那篇博客是用ISO-8859-1字符集显示的，字符集不能识别俄语，泰文或者中文，只能识别小部分希腊文。如果你把任意拷贝来的东西粘贴并且提交，现代浏览器会尝试把它们转化为HTML实体集，比如把Я转化为<code>&amp;#1071;</code></p>
			<p>转化后的结果会存在数据库中，当评论被显示在网页上时是没有问题的，但是当你想把它导出为PDF或者输出在Email中，又或者在数据库中进行搜索时，问题就来了。</p>









			</div>
			<div class="span4"></div>
			<div class="span1"></div>
		</div>
	</div>
	<script type="text/javascript" src="js/bootstrap.js"></script>
</body>
</html>