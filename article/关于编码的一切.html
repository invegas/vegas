<!DOCTYPE html>
<html>
<head>
	<title>谈javascript变量声明</title>
	<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
	<link rel="stylesheet" type="text/css" href="prettify/sunburst.css">
	<script type="text/javascript" src="prettify/prettify.js"></script>	
	<style type="text/css">
		body {
			font-family: '微软雅黑';
		}
		blockquote p{
			font-style: italic;
			font-size: 12px;
		}
	</style>
</head>
<body onload="prettyPrint()">
	<div class="container">
		<div class="row">
			<div class="span1"></div>
			<div class="span6">
				<h3>Javascript: 你真的了解函数声明吗？</h3>
			</div>
			<div class="span4"></div>
			<div class="span1"></div>
		</div>		
		<div class="row">
			<div class="span1"></div>
			<div class="span6">




			<p>有感于，我们每天用各种的编辑器，嘴里喊着utf-8，BOM头，gbk，encode，decode，却鲜有人知道它们的由来和为什么这样做（好吧，也有可能就我一个人不知道）。最近找了很多资料，在这里做一个整理，和大家分享。</p>
			<strong>关于Unicode,UTF8,Character Sets的前生今世</strong>
			<strong>ASC II</strong>
			<p>Long long time age, 总所周知电脑只能处理数字而不能处理字符，所以把哪些字符由哪些数字表示统一起来（而不是每台电脑有每台电脑的标准）非常重要。</p>
			<p>比如说我的电脑用1表示A，2表示B，3表示C，诸如此类。而你的电脑用0表示A，1表示B，2表示C等等。那么当我发送给你一条内容为“HELLO”的消息时，数字8,5,12,12,15就通过光缆传输到了你那，但因为数字表示的字符不同，当你收到这串数字时，你会把它翻译(<strong>decode</strong>)成“IFMMP”。为了有效的交流，我们必须对如何编码(encoding)这些字符达成一致。</p>
			<p>终于，在十九世纪60年代，美国标准协会(American Standards Association)发明了7位(7-bit)编码方式，称为美国信息交换标准码(American Standard Code for Information Interchange)，就是我们熟知的<strong>ASCII</strong>。在这种编码标准下，HELLO表示的数字是72,69,76,76,79，并且会以二进制1001000 1000101 1001100 1001100 1001111的形式进行传输。7位编码一共提供了128种可能，从0000000到1111111。那时所有的拉丁字符
			的大小写，通用的标点，缩进，空格和其它一些控制符都能在ASCII中有一席之地。在1968年，美国总统Lyndon Johnson宣布所有的电脑必须使用和能读懂ASCII标准，ASCII成为官方标准。</p>
			<strong>8-bit</strong>
			<p>电报一类的工具当然乐于使用七位编码去传输信息，但是到了七十年代，电脑的处理器更乐意与2的次方打交道——他们已经可以用8位来存储字符，也就是说这将提供256种可能。</p>
			<p>一个8位字符被存储的数字最大是255，但是ASCII标准最大只到127。这就意味着从128到255被空了出来。IBM用这些多余的数字来存储一些形状，希腊字母。比如200代表一个盒子的左下角╚，244代表小写的希腊字母<strong>α</strong>。这种编码方式的所有字符编码都列在<a target="_blank" href="http://en.wikipedia.org/wiki/Code_page_437">代码页(Code page)437中</a></p>
			<p>无论如何，不像ASCII标准，128至255的字符序列从来都没有被标准化。不同国家都用自己的字母表来填充这些多余的序列。不是所人都同意224代表<strong>α</strong>，甚至希腊人自己都有分歧，因为在希腊另一代码页737中，224代表小写<strong>ω</strong>。相当数量的新的代码页的出现，比如俄罗斯的IBM电脑使用的是代码页885,224代表的是西里尔(Cyrillic)字母<strong>Я</strong>。</p>
			<p>在存在分歧的情况下，十九世纪八十年代微软也推出了自己的代码页，在西里尔文代码页Windows-1251中，224代表西里尔文字母<strong>a</strong>，而之前的<strong>Я</strong>的位置是223.</p>
			<p>到了九十年代末期，大家做了一次标准化的尝试。15种8位字符集被推出，涵盖不同的字母表，比如西里尔文，阿拉伯文，希伯来文，土耳其文和泰文。它们被称为<a href="" target="_blank"> ISO-8859-1 up to ISO-8859-16</a>(字母12被抛弃)。在西里尔标准ISO-8859-5中，224代表字母<strong>р</strong>，而<strong>Я</strong>的位置是207.
			</p>
			<p>如果一位俄罗斯朋友发给你一份文档，你必须知道他使用的是哪一种字符集。文档本身只是数字序列而已，224代表的可能是Я,a 或者р。如果用错了字符集打开这份文档，那会是非常糟糕的一件事。</p>
			<strong>给1990年做个总结</strong>
			<p>在1990年左右的情况大致是这样的，文档可以用不同的语言书写，保存，并且在不同语言间交换。但是<strong>你必须得知道他们用的是哪一种字符集</strong>。当然更不可能在一份文档中用多种语言。像中文和日文只能用完全不同的编码体系。
			</p>
			<p>终于，互联网出现了！国际化和全球化让这个问题被放的更大，一个新的标准亟需出现。</p>
			<strong>Unicode来拯救人类了</strong>
			<p>从八十年代后期开始，一种新的标准已经被提出。它能给每一种语言中的每一个字符赋予唯一的标识，当然远远大于256.它被称为<strong>Unicode</strong>，至今为止它的版本是6.1，包括了超过110000个字符。</p>
			<p>Unicode的头128个字符与ASCII一致。128至255则包含了货币符号，常用符号，还有<a target="_blank" href="http://en.wikipedia.org/wiki/Diacritic">附加符号和变音符(accented characters)</a>。大部分都是借鉴自ISO-8859-1。在编号256之后，还有更多的变音符。在编号880之后开始进入希腊文字符集，然后是西里尔文，希伯来文，阿拉伯文，印度语，泰文。中文，日文和韩文从11904开始，其中也夹杂了其他的语言。</p>
			<p>毫不含糊的说这的确是一件福利，每一个字符都由属于自己独一无二的数字表示。西里尔文的Я永远是1071，希腊文的α永远是945.而224永远是à，H任然是72.注意官方书写的Unicode编码是以U+为开头的十六进制表示。所以H的正确写法应该是U+48而不是72（把十六进制转化为十进制：4*16+8=72）</p>
			<p>最主要的问题是超出256的那部分。想当然8位已经容不下这些字符了。但无论如何Unicode并不是一个字符集或者是代码页。所以这也不是Unicode制定协会的问题。他们只是给出了一个解决问题的想法而剩下实际操作的问题则留给其他人去办了。下两节我们会讨论这个问题。</p>
			<strong>浏览器中的Unicode</strong>
			<p>8位已经容不下Unicode了，甚至16也已经容不下，虽然只有110116个</p>








			</div>
			<div class="span4"></div>
			<div class="span1"></div>
		</div>
	</div>
	<script type="text/javascript" src="js/bootstrap.js"></script>
</body>
</html>